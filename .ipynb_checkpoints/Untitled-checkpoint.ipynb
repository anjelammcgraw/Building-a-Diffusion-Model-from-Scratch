{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc86ac66-c258-4837-8361-99c357c39bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc59966-cf35-4086-8e5b-b4a697f8ace4",
   "metadata": {},
   "source": [
    "Diffusion Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af1206-0aab-4a85-a8d6-90e0abf576a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52610d99-145b-460d-9344-e127c123796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "\n",
    "        # Create a list of layers for the upsampling block\n",
    "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "\n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
    "        x = torch.cat((x, skip), 1)\n",
    "\n",
    "        # Pass the concatenated tensor through the sequential model and return the output\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "\n",
    "        # Create a list of layers for the downsampling block\n",
    "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n",
    "\n",
    "        # Use the layers to create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the sequential model and return the output\n",
    "        return self.model(x)\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
    "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # define the layers for the network\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "\n",
    "        # create a PyTorch sequential model consisting of the defined layers\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input tensor\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        # apply the model layers to the flattened tensor\n",
    "        return self.model(x)\n",
    "\n",
    "def unorm(x):\n",
    "    # unity norm. results in range of [0,1]\n",
    "    # assume x (h,w,3)\n",
    "    xmax = x.max((0,1))\n",
    "    xmin = x.min((0,1))\n",
    "    return(x - xmin)/(xmax - xmin)\n",
    "\n",
    "def norm_all(store, n_t, n_s):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    nstore = np.zeros_like(store)\n",
    "    for t in range(n_t):\n",
    "        for s in range(n_s):\n",
    "            nstore[t,s] = unorm(store[t,s])\n",
    "    return nstore\n",
    "\n",
    "def norm_torch(x_all):\n",
    "    # runs unity norm on all timesteps of all samples\n",
    "    # input is (n_samples, 3,h,w), the torch image format\n",
    "    x = x_all.cpu().numpy()\n",
    "    xmax = x.max((2,3))\n",
    "    xmin = x.min((2,3))\n",
    "    xmax = np.expand_dims(xmax,(2,3))\n",
    "    xmin = np.expand_dims(xmin,(2,3))\n",
    "    nstore = (x - xmin)/(xmax - xmin)\n",
    "    return torch.from_numpy(nstore)\n",
    "\n",
    "def gen_tst_context(n_cfeat):\n",
    "    \"\"\"\n",
    "    Generate test context vectors\n",
    "    \"\"\"\n",
    "    vec = torch.tensor([\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
    "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0]]      # human, non-human, food, spell, side-facing\n",
    "    )\n",
    "    return len(vec), vec\n",
    "\n",
    "def plot_grid(x,n_sample,n_rows,save_dir,w):\n",
    "    # x:(n_sample, 3, h, w)\n",
    "    ncols = n_sample//n_rows\n",
    "    grid = make_grid(norm_torch(x), nrow=ncols)  # curiously, nrow is number of columns.. or number of items in the row.\n",
    "    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n",
    "    print('saved image at ' + save_dir + f\"run_image_w{w}.png\")\n",
    "    return grid\n",
    "\n",
    "def plot_sample(x_gen_store,n_sample,nrows,save_dir, fn,  w, save=False):\n",
    "    ncols = n_sample//nrows\n",
    "    sx_gen_store = np.moveaxis(x_gen_store,2,4)                               # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
    "    nsx_gen_store = norm_all(sx_gen_store, sx_gen_store.shape[0], n_sample)   # unity norm to put in range [0,1] for np.imshow\n",
    "\n",
    "    # create gif of images evolving over time, based on x_gen_store\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True,figsize=(ncols,nrows))\n",
    "    def animate_diff(i, store):\n",
    "        print(f'gif animating frame {i} of {store.shape[0]}', end='\\r')\n",
    "        plots = []\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                axs[row, col].clear()\n",
    "                axs[row, col].set_xticks([])\n",
    "                axs[row, col].set_yticks([])\n",
    "                plots.append(axs[row, col].imshow(store[i,(row*ncols)+col]))\n",
    "        return plots\n",
    "    ani = FuncAnimation(fig, animate_diff, fargs=[nsx_gen_store],  interval=200, blit=False, repeat=True, frames=nsx_gen_store.shape[0])\n",
    "    plt.close()\n",
    "    if save:\n",
    "        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "        print('saved gif at ' + save_dir + f\"{fn}_w{w}.gif\")\n",
    "    return ani\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sfilename, lfilename, transform, null_context=False):\n",
    "        self.sprites = np.load(sfilename)\n",
    "        self.slabels = np.load(lfilename)\n",
    "        print(f\"sprite shape: {self.sprites.shape}\")\n",
    "        print(f\"labels shape: {self.slabels.shape}\")\n",
    "        self.transform = transform\n",
    "        self.null_context = null_context\n",
    "        self.sprites_shape = self.sprites.shape\n",
    "        self.slabel_shape = self.slabels.shape\n",
    "\n",
    "    # Return the number of images in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sprites)\n",
    "\n",
    "    # Get the image and label at a given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the image and label as a tuple\n",
    "        if self.transform:\n",
    "            image = self.transform(self.sprites[idx])\n",
    "            if self.null_context:\n",
    "                label = torch.tensor(0).to(torch.int64)\n",
    "            else:\n",
    "                label = torch.tensor(self.slabels[idx]).to(torch.int64)\n",
    "        return (image, label)\n",
    "\n",
    "    def getshapes(self):\n",
    "        # return shapes of data and labels\n",
    "        return self.sprites_shape, self.slabel_shape\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # from [0,255] to range [0.0,1.0]\n",
    "    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c55cd-eac9-4f5f-9510-f0f586d9fd66",
   "metadata": {},
   "source": [
    "Setting Things Up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04004d49-d308-49d5-9cfb-97128ee730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "\n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f183d-672d-43cf-b4b0-7bc4b391c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7258715-9da2-423b-a3a4-f8d9067766ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c307c-24e1-4a5c-97ea-08033bae8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResidualConvBlock class\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.is_res = in_channels != out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.is_res:\n",
    "            res = self.res_conv(res)\n",
    "\n",
    "        out += res\n",
    "        return self.relu(out)\n",
    "\n",
    "# Define the ContextUnet class\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  # assume h == w. must be divisible by 4, so 28, 24, 20, 16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat)\n",
    "\n",
    "        # Initialize the down-sampling path of the U-Net with two levels\n",
    "        self.down1 = UnetDown(n_feat, n_feat)        # down1 #[10, 256, 8, 8]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)    # down2 #[10, 256, 4,  4]\n",
    "\n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat)\n",
    "\n",
    "        # Initialize the up-sampling path of the U-Net with three levels\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, self.h//4, self.h//4), # up-sample\n",
    "            nn.GroupNorm(8, 2 * n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # Convert t to float\n",
    "        t = t.float()\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        x = self.init_conv(x)\n",
    "        # pass the result through the down-sampling path\n",
    "        down1 = self.down1(x)       #[10, 256, 8, 8]\n",
    "        down2 = self.down2(down1)   #[10, 256, 4, 4]\n",
    "\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        #print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7017e7-8ae4-4ca3-ac3d-6ca1e4b803c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad96aa5-736b-4a1c-b472-caf1b5d40339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8c781-8a39-40d3-868e-dc719775220d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "231b2616-5eb7-4f12-b988-5d8e33f67f8a",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cdd43-45ae-48df-808a-ad4fb61dde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f184ff5-ac6c-4fbe-b5ac-71e0b8425cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(nn_model.state_dict(), os.path.join(save_dir, 'model_trained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a842e-757d-4c41-9a44-a0df185a6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model weights and set to eval mode\n",
    "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_trained.pth\", map_location=device))\n",
    "nn_model.eval()\n",
    "print(\"Loaded in Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a3f63-f2d0-4de9-a2a2-04980c8b0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    nn_model.eval()  # Ensure the model is in evaluation mode\n",
    "    samples = torch.randn(n_sample, 3, 32, 32).to(device)  # Assuming 3 channels (e.g., RGB) and 32x32 image size\n",
    "    intermediate_ddpm = []\n",
    "\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        t = torch.full((n_sample,), i, device=device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eps = nn_model(samples, t)  # predict noise e_(x_t,t)\n",
    "            samples = denoise_add_noise(samples, i, eps, z=None)  # Adjust according to your function definition\n",
    "\n",
    "        if i % save_rate == 0 or i == timesteps or i < 8:\n",
    "            intermediate_ddpm.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    return samples, intermediate_ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc3499-e4c8-4d31-a966-06bf656c54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a222c8a-4ee7-4f1a-8a39-0c56475e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demonstrating incorrectly sample without adding extra noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869cc96-7fcc-4b32-b04b-fc321adc3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrectly sample without adding in noise\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_incorrect(n_sample):\n",
    "    # x_T ~ N(0, 1), sample initial noise\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
    "\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = []\n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # don't add back in noise\n",
    "        z = 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i%20==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24145925-20fc-479e-9527-d95fcd0cd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate = sample_ddpm_incorrect(32)\n",
    "animation = plot_sample(intermediate,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
